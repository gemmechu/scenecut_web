<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SceneCut: Semi-automatic segmentation, reconstruction and separation of 3D objects</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" type="image/png" href="">
  
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SceneCut: Semi-automatic segmentation, reconstruction and separation of 3D objects</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Gemmechu Hassena</a>,</span>
            <span class="author-block">
              <a href="">Jonathan Moon</a>,</span>
            <span class="author-block">
              <a href="">Ryan Fujii</a>,
            </span>
            <span class="author-block">
              <a href="">Andrew Yuen</a>,
            </span>
            <span class="author-block">
              <a href="">Noah Snavely</a>,
            </span>
            <span class="author-block">
              <a href="">Steve Marschner</a>,
            </span>
            <span class="author-block">
              <a href="">Bharath Hariharan</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Cornell University</span>
            
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
             
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/method.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Implicit neural fields have made remarkable progress in reconstructing 3D surfaces from multiple images; however, they encounter challenges when it comes to separating individual objects within a scene. Previous work has attempted to tackle this problem by introducing a framework to train separate signed distance fields (SDFs) simultaneously for each of N objects and using a regularization term to prevent objects from overlapping. 
However, all of these methods require segmentation masks to be provided, which are not always readily available. 

          </p>
          <p>
            We introduce our method, SceneCut, to tackle the problem of object separation from just click input in a single view. 
Given posed multi-view images and a set of user-input clicks to prompt segmentation of the individual objects, our method decomposes the scene into separate objects and reconstructs a high-quality 3D surface for each one. 
We introduce a loss function that prevents floaters and avoids inappropriate carving-out due to occlusion.  

          </p>
          <p>
            In addition, we introduce a novel scene initialization method that significantly speeds up the process while preserving geometric details compared to previous approaches.
Despite requiring neither ground truth masks nor monocular cues, our method outperforms baselines both qualitatively and quantitatively. In addition, we introduce a new benchmark dataset for evaluation. 

          </p>
        </div>
      </div>
    </div>
   
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/scene16.mp4"
                    type="video/mp4">
          </video>
        </div>
    
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/scene3.mp4"
                    type="video/mp4">
          </video>
        </div>
       
       
       
        <div class="item item-toby">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/scan5.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
         
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Mask Propagation</h2>
       
      </div>
     
    </div>
    <p>We propose automatic mask generation from just user clicks, which generates segmentation masks for all the input views. in the first iteration, a user clicks a point on each object to generate a per-object anchor mask, which are then unprojected into 3D (here, we only show unprojected 3D points for the bottom can). 
      These 3D points are subsequently projected back into each image view, while checking for occlusions. The projected points serve as seeds for SAM to generate masks for each object (bottom and top cans, door stop).
       To combine these individual segmentation masks into a single image, we use a depth ordering technique.
       In the next iterations, all views are used as anchor masks, allowing the pipeline to cover previously unseen regions.</p>
    <img src="./static/images/mask_prop_step.png" 
    class="interpolation-image"
    alt="Interpolate start reference image."/>
    
     <br> 
    
    <div id="results-carousel" class="carousel results-carousel">
      <div class="item item-steve">
        <video poster="" id="steve"  autoplay controls muted loop playsinline height="100%">
          <source src="./static/videos/mask_prop/scan_9.mp4"
                  type="video/mp4">
        </video>
      </div>
      <div class="item item-steve">
        <video poster="" id="steve"  controls muted loop playsinline height="100%">
          <source src="./static/videos/mask_prop/scan_16.mp4"
                  type="video/mp4">
        </video>
      </div>
  
      <div class="item item-steve">
        <video poster="" id="steve"  controls muted loop playsinline height="100%">
          <source src="./static/videos/mask_prop/scan_14.mp4"
                  type="video/mp4">
        </video>
      </div>
      <div class="item item-steve">
        <video poster="" id="steve"  controls muted loop playsinline height="100%">
          <source src="./static/videos/mask_prop/scan_25.mp4"
                  type="video/mp4">
        </video>
      </div>
      <div class="item item-steve">
        <video poster="" id="steve"  controls muted loop playsinline height="100%">
          <source src="./static/videos/mask_prop/scan_3.mp4"
                  type="video/mp4">
        </video>
      </div>
      
      <div class="item item-steve">
        <video poster="" id="steve"  controls muted loop playsinline height="100%">
          <source src="./static/videos/mask_prop/scan_28.mp4"
                  type="video/mp4">
        </video>
      </div>
    </div>
   
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Dataset</h2>
       
      </div>
     
    </div>
   
    <img src="./static/images/dataset2.png" 
    class="interpolation-image"
    alt="Interpolate start reference image."/>
    
    <p>
      We introduce a new benchmark for 3D scene decomposition techniques, consisting of 30 real-world scenes and 5 synthetically generated ones. The scenes contains different combinations of objects in close contact, and we provide a high-quality complete mesh of each object. 
      Previous scene decomposition techniques evaluate their methods on benchmark datasets like Replica and ScanNet. A critical limitation of these is that they do not offer complete ground truth geometries for the reconstructed objects. 
      More specifically, per-object meshes are extracted from the full ground truth mesh of the indoor scene by cropping the ground truth mesh with the semantic masks and therefore lack completeness in regions occluded by other objects. 
    </p> 
    <br> 
    <img src="static/images/scenes_1.png" alt="Image 1">
    <img src="static/images/scenes_2.png" alt="Image 2">
   
  </div>
</section>
<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Acknowledgements</h2>
    We would like to thank Qianyi Wu, Jiepeng Wang, Aditya Chetan and Milky Hassena.  
  </div>
</section>
<footer class="footer">
  <div align="center" class="container">
    <div class="columns is-centered">
        <div class="content">
            This website is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
        </div>
      </div>
    </div>
</footer>


</body>
</html>
